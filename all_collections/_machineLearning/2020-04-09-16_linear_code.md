---
title: (Machine) 16. 선형회귀 코드 예제
excerpt: "선형회귀 관련 이론"
categories:
  - Machine
tags:
  - Machine
  - 머신러닝
last_modified_at: 2020-04-09
---

## 1. Linear Regression(선형회귀)
- 선형회귀 가설 식 :
$$
H(x) = Wx + b
$$ 
- Cost 식 :$$
cost(W, b) = \frac{1}{m}\sum_{i=1}^m{(H( x^{(i)}) - y^{(i)})^2}
$$
- Cost식 설명 ▼

1. $$ H( x^{(i)}) - y^{(i)})^2 $$   :  에러를 구하고, 제곱을 해서 음의 값을 뺀 다음
2. $$
\sum_{i=1}^m
$$     : 모두다 더해서
3. $$ \frac{1}{m} $$     : 데이터갯수 m으로 나눈 값으로 정의

## 2. Hypothesis && cost 코드
~~~python
import tensorflow as tf

x_data = [1,2,3,4,5]
y_data = [1,2,3,4,5]

W = tf.Variable(2.9)
b = tf.Variable(0.5)

# hypothesis 구하는 코드
hypothesis = W * x_data + b
print(hypothesis)

# cost 구하는 코드
cost = tf.reduce_mean(tf.square(hypothesis - y_data))
~~~

## 3. tensorflow 코드
- tf.reduce_mean() = 평균 구함 [차원인 제거됨] 
- tf.square(3)     = 제곱 함수
- tf.GradientTape()  = with구문하고 같이 사용한다.
  with 구문안에 있는 변수들의 정보들을 tape에 기록해 놓는다.



## 4. Gradient descent
- 경사하강 알고리즘 :
$$
\begin{align}
minimize\ cost(W,b)
\end{align}
$$

~~~python
# Learning_rate initialize
learning_rate = 0.01

# Gradient descent
with tf.GradientTape() as tape: #with 구문안에 있는 변수들의 정보들을 tape에 기록해 놓는다.
    hypothesis = W * x_data + b
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))

W_grad, b_grad = tape.gradient(cost, [W, b]) # 개별 미분값(기울기값)을 구해서 튜플로 반환한다.

W.assign_sub(learning_rate * W_grad) # 기울기를 얼만큼 반영할 것인가를 러닝rate를 곱해서 표현
b.assign_sub(learning_rate * W_grad)
~~~